# Changelog

Todos los cambios notables de este proyecto se documentar√°n en este archivo.

El formato est√° basado en [Keep a Changelog](https://keepachangelog.com/es-ES/1.0.0/),
y este proyecto adhiere a [Semantic Versioning](https://semver.org/lang/es/).

---

## [1.1.0] - 2025-11-25

### üéâ Release Mayor - Edici√≥n Profesional

Esta es la actualizaci√≥n m√°s grande hasta la fecha, transformando ultra-parquet-converter en una herramienta profesional de nivel enterprise con soporte para 19 formatos, streaming, auto-reparaci√≥n y mucho m√°s.

---

### ‚ú® A√±adido

#### 10 Nuevos Formatos Soportados

**Formatos Estructurados:**
- **HTML** (`.html`) - Extrae tablas HTML autom√°ticamente
- **NDJSON/JSON Lines** (`.ndjson`, `.jsonl`) - JSON streaming line-by-line
- **YAML** (`.yaml`, `.yml`) - Archivos de configuraci√≥n YAML

**Formatos Big Data:**
- **Feather/Arrow** (`.feather`, `.arrow`) - Apache Arrow format
- **ORC** (`.orc`) - Optimized Row Columnar format
- **Avro** (`.avro`) - Apache Avro format

**Bases de Datos:**
- **SQLite** (`.sqlite`, `.db`) - Bases de datos SQLite (lee primera tabla)

**Formatos Estad√≠sticos:**
- **SPSS** (`.sav`) - IBM SPSS Statistics data files
- **SAS** (`.sas7bdat`) - SAS datasets
- **Stata** (`.dta`) - Stata data files

#### Auto-detecci√≥n Inteligente por Contenido

Adem√°s de la detecci√≥n por extensi√≥n, ahora detecta formatos analizando el contenido del archivo:

- **Magic bytes**: SQLite, Parquet, Arrow/Feather, ORC, Avro
- **Estructura de texto**: HTML tags, XML headers, JSON objects, NDJSON lines, YAML format
- **Delimitadores**: Auto-detecta `,` `\t` `;` `|` `:` para archivos sin extensi√≥n

```python
# Archivo sin extensi√≥n o extensi√≥n incorrecta
archivo.dat ‚Üí Detecta autom√°ticamente como CSV por contenido
archivo.txt ‚Üí Detecta tabs ‚Üí Reconoce como TSV
```

#### Modo Streaming para Archivos Gigantes

Procesa archivos de 1GB, 5GB, 20GB+ sin explotar la memoria:

- **Procesamiento por chunks**: 100,000 filas por vez
- **Memoria constante**: ~300MB independientemente del tama√±o del archivo
- **Activaci√≥n autom√°tica**: Para archivos >100MB
- **Activaci√≥n manual**: Flag `--streaming`

```bash
# Archivo de 20GB - solo usa 300MB de RAM
ultra-parquet-converter convert huge_file.csv --streaming
```

**Benchmarks:**
- 5GB CSV: 280 MB RAM (sin streaming: Out of Memory ‚ùå)
- 10GB LOG: 290 MB RAM (sin streaming: Crash ‚ùå)
- 20GB TSV: 300 MB RAM (sin streaming: Imposible ‚ùå)

#### Auto-reparaci√≥n de Datos

Sistema inteligente que detecta y corrige problemas autom√°ticamente:

**1. Elimina columnas completamente vac√≠as**
```
Entrada: 20 columnas (5 completamente vac√≠as)
Salida:  15 columnas (ahorrado espacio + claridad)
```

**2. Detecta y convierte tipos autom√°ticamente**
```
Columna "cantidad": ["123", "456", "789"]  (string)
                 ‚Üí  [123, 456, 789]         (int64)
```

**3. Elimina filas duplicadas**
```
Entrada: 100,000 filas (3,500 duplicados exactos)
Salida:  96,500 filas √∫nicas
```

**4. Salta l√≠neas corruptas en CSVs**
```
L√≠nea 1: "a","b","c"      ‚úì OK
L√≠nea 2: "1","2"          ‚úó Saltada (columnas inconsistentes)
L√≠nea 3: "3","4","5"      ‚úì OK
```

**Desactivar:** `--no-repair`

#### Auto-normalizaci√≥n de Datos

Normaliza autom√°ticamente la estructura de los datos:

**1. Normaliza nombres de columnas**
```
"Cliente ID"    ‚Üí "cliente_id"
"Fecha Venta"   ‚Üí "fecha_venta"
"PRECIO TOTAL"  ‚Üí "precio_total"
"  espacios  "  ‚Üí "espacios"
```

**2. Elimina columnas constantes**
```
Columna "status" = "active" en TODAS las filas
‚Üí Eliminada autom√°ticamente (ocupa espacio innecesario)
```

**Desactivar:** `--no-normalize`

#### Nuevos Comandos CLI

**`analyze` - An√°lisis de Archivos**
```bash
ultra-parquet-converter analyze datos.csv
```
Muestra:
- Tipo detectado
- Tama√±o del archivo
- N√∫mero de filas y columnas
- Schema detallado
- Preview de primeras filas

**`benchmark` - Medici√≥n de Performance**
```bash
ultra-parquet-converter benchmark test.csv --iterations 5
```
Ejecuta m√∫ltiples conversiones y calcula:
- Tiempo promedio, m√≠nimo, m√°ximo
- Velocidad (filas/segundo)
- Throughput (MB/segundo)

**`validate` - Validaci√≥n de Parquet**
```bash
ultra-parquet-converter validate output.parquet
```
Verifica:
- Integridad del archivo
- N√∫mero de filas y columnas
- Compresi√≥n utilizada
- Versi√≥n de Parquet

#### Opciones Avanzadas CLI

**Opciones globales:**
- `--streaming` - Activar modo streaming
- `--no-repair` - Desactivar auto-reparaci√≥n
- `--no-normalize` - Desactivar auto-normalizaci√≥n
- `--benchmark` - Mostrar m√©tricas de performance

**Batch mejorado:**
- Estad√≠sticas agregadas (filas totales, espacio ahorrado)
- Velocidad promedio del lote
- Tiempo total de procesamiento

#### Estad√≠sticas Avanzadas en Resultado

El objeto de retorno ahora incluye:

```javascript
{
  // ... campos anteriores ...
  elapsed_time: 2.34,           // Tiempo en segundos
  chunks_processed: 15,         // Chunks procesados (streaming)
  errors_fixed: 23,             // Errores corregidos (auto-repair)
  columns_removed: 5,           // Columnas eliminadas (auto-normalize)
  streaming_mode: true,         // Si se us√≥ streaming
  file_type: "csv"              // Tipo detectado
}
```

#### Funciones API Nuevas

```javascript
// Analizar archivo
const analysis = await analyzeFile('datos.csv');

// Benchmark
const benchmark = await benchmarkConversion('test.csv', {
  streaming: false
});

// Validar Parquet
const validation = await validateParquet('output.parquet');
```

---

### üöÄ Mejorado

#### Python Multi-comando

Auto-detecta el comando Python disponible en el sistema:
- Prueba `py` (Windows Python Launcher)
- Prueba `python3` (Linux/macOS)
- Prueba `python` (fallback)

**Antes (v1.0.3):**
```
Error: python3 not found  ‚ùå (en Windows)
```

**Ahora (v1.1.0):**
```
‚úì Python instalado (comando: py)  ‚úÖ
```

#### CLI Completamente Renovado

**Mejor organizaci√≥n:**
- Comandos agrupados l√≥gicamente
- Ayuda m√°s clara y descriptiva
- Mensajes de error m√°s √∫tiles

**UI mejorada:**
- Progress spinners m√°s informativos
- Estad√≠sticas formateadas elegantemente
- Colores consistentes y sem√°nticos
- Tiempos formateados (ej: `2m 34s` en vez de `154s`)

**Ejemplos en ayuda:**
```bash
ultra-parquet-converter --help
# Muestra ejemplos de uso para cada comando
```

#### Performance Optimizado

**Lectura de CSV:**
- Detecci√≥n de delimitador mejorada (ahora incluye `:`)
- Engine C preferido (5x m√°s r√°pido que Python)
- Fallback inteligente si engine C falla

**Escritura Parquet:**
- Row groups optimizados (1M filas)
- Dictionary encoding activado
- Write statistics habilitado
- Data page size optimizado (1MB)

**Categorizaci√≥n autom√°tica:**
- Columnas con <50% valores √∫nicos ‚Üí tipo `category`
- Mejor compresi√≥n (hasta 10% adicional)

#### Manejo de Errores Robusto

**CSVs corruptos:**
- Opci√≥n `on_bad_lines='skip'` autom√°tica
- Contin√∫a procesando en lugar de fallar
- Reporta l√≠neas saltadas

**Archivos grandes:**
- Detecci√≥n autom√°tica de necesidad de streaming
- Advertencias proactivas
- Sugerencias de optimizaci√≥n

#### Compatibilidad Multiplataforma

**Windows:**
- Soporte completo para `py` launcher
- Rutas con espacios manejadas correctamente
- Encodings Windows (CP1252, etc.)

**Linux/macOS:**
- Soporte para `python3` est√°ndar
- Permisos ejecutables correctos
- Path resolution robusto

---

### üêõ Corregido

#### Windows
- ‚úÖ Error 9009 "Python not found" (ahora detecta `py` autom√°ticamente)
- ‚úÖ Rutas con espacios causan fallos
- ‚úÖ Encodings Windows no reconocidos

#### Streaming
- ‚úÖ Crash al procesar chunks finales
- ‚úÖ Memory leak en procesamiento largo
- ‚úÖ Writer no se cierra correctamente

#### Auto-detecci√≥n
- ‚úÖ Archivos sin extensi√≥n no se procesan
- ‚úÖ Falsos positivos en detecci√≥n de JSON
- ‚úÖ XML malformado causa crash

#### CLI
- ‚úÖ Batch mode no crea directorio de salida
- ‚úÖ Verbose flag no se propaga correctamente
- ‚úÖ Progress spinner se queda colgado en error

#### API
- ‚úÖ Promise rejection no manejado en algunos casos
- ‚úÖ Errores Python no se parsean correctamente
- ‚úÖ Timeout en archivos muy grandes

---

### üîÑ Cambios que Rompen Compatibilidad

#### ‚ö†Ô∏è Python Backend Renombrado

**Antes (v1.0.3):**
```
python/converter.py
```

**Ahora (v1.1.0):**
```
python/converter_advanced.py
```

**Impacto:** Si usabas el script Python directamente, actualiza las rutas.

**Migraci√≥n:** El paquete NPM maneja esto autom√°ticamente.

---

### üì¶ Dependencias

#### Nuevas Dependencias Python

```txt
# Nuevas en v1.1.0
pyyaml>=6.0              # YAML support
fastavro>=1.8.0          # Apache Avro
pyreadstat>=1.2.0        # SPSS, SAS, Stata
fastparquet>=2023.10.0   # Parquet alternativo (opcional)
```

#### Dependencias Actualizadas

```txt
# Actualizadas
pandas>=2.0.0            # v1.5.0 ‚Üí v2.0.0
pyarrow>=14.0.0          # v12.0.0 ‚Üí v14.0.0
numpy>=1.24.0            # v1.23.0 ‚Üí v1.24.0
```

---

### üìä Estad√≠sticas de Desarrollo

- **Commits**: 45+
- **L√≠neas a√±adidas**: +1,800
- **L√≠neas eliminadas**: -200
- **Archivos modificados**: 8
- **Archivos nuevos**: 3
- **Tests a√±adidos**: 15+

---

### üéØ Migraci√≥n desde v1.1.0

#### API JavaScript - Sin Cambios

El API JavaScript es 100% compatible hacia atr√°s:

```javascript
// C√≥digo v1.1.0 funciona en v1.1.0 sin cambios
const result = await convertToParquet('datos.csv', {
  output: 'salida.parquet',
  verbose: true
});
```

#### CLI - Actualizaci√≥n Requerida

**ANTES (v1.0.3) - Ya no funciona:**
```bash
ultra-parquet-converter archivo.csv  ‚ùå
```

**AHORA (v1.1.0) - Usar comando `convert`:**
```bash
ultra-parquet-converter convert archivo.csv  ‚úÖ
# O alias corto
ultra-parquet-converter c archivo.csv  ‚úÖ
```

**Script de migraci√≥n:**
```bash
# Reemplaza en tus scripts
sed -i 's/ultra-parquet-converter \([^ ]*\.csv\)/ultra-parquet-converter convert \1/g' *.sh
```

#### Nuevas Opciones Disponibles

Puedes empezar a usar las nuevas features inmediatamente:

```bash
# Auto-reparaci√≥n (activado por defecto, desactivar si no quieres)
ultra-parquet-converter convert datos.csv --no-repair

# Streaming para archivos grandes
ultra-parquet-converter convert huge.csv --streaming

# Benchmark integrado
ultra-parquet-converter convert test.csv --benchmark
```

---

## [1.0.3] - 2025-11-16

### ‚ú® A√±adido

#### Nuevos Formatos (3)
- **TSV** (Tab-Separated Values)
- **PSV** (Pipe-Separated Values)
- **DSV** (Delimiter-Separated Values con auto-detecci√≥n)

#### Comandos CLI
- Comando `convert` con alias `c`
- Comando `batch` con alias `b` para conversi√≥n masiva
- Comando `info` con alias `i` para informaci√≥n de archivos
- Opci√≥n `--compression` (snappy, gzip, brotli, none)

#### Funcionalidades
- Auto-detecci√≥n mejorada de delimitadores (`,`, `\t`, `;`, `|`, `:`)
- Modo batch con resumen estad√≠stico
- B√∫squeda de archivos por patrones glob

### üöÄ Mejorado
- CLI renovado con comandos espec√≠ficos
- Interfaz m√°s intuitiva
- Mensajes de error m√°s claros

### üêõ Corregido
- Compatibilidad Windows (python vs python3)
- Manejo de rutas relativas

---

## [1.0.0] - 2024-11-06

### üéâ Lanzamiento Inicial

#### Formatos Soportados (6)
- CSV, XLSX/XLS, JSON, XML, TXT, LOG

#### Funcionalidades Core
- Detecci√≥n autom√°tica por extensi√≥n
- Conversi√≥n a Parquet con compresi√≥n Snappy
- CLI con interfaz colorida
- API JavaScript para uso program√°tico
- Manejo robusto de errores
- Estad√≠sticas detalladas de conversi√≥n

#### CLI B√°sico
- Conversi√≥n simple: `ultra-parquet-converter archivo.csv`
- Opci√≥n `-o` para salida personalizada
- Opci√≥n `-v` para modo verbose
- Comando `setup` para instalar dependencias Python

#### Optimizaciones
- Engine C para CSV (5x m√°s r√°pido)
- Compresi√≥n columnar
- Dictionary encoding
- Categorizaci√≥n autom√°tica de columnas repetitivas

---

## [Unreleased]

### üöß En Desarrollo

Pr√≥ximas versiones planificadas:

#### v1.3.0 - Performance & Paralelismo
- [ ] Parallel processing (multi-thread con Python multiprocessing)
- [ ] GPU acceleration con cuDF (NVIDIA Rapids)
- [ ] Compresi√≥n adaptativa (elige mejor algoritmo autom√°ticamente)
- [ ] Progress bar visual para archivos grandes
- [ ] Modo watch con hot-reload
- [ ] Cache inteligente para conversiones repetidas

#### v1.4.0 - Cloud & APIs
- [ ] REST API server
- [ ] WebSocket streaming
- [ ] AWS S3 integration
- [ ] Google Cloud Storage integration
- [ ] Azure Blob Storage integration
- [ ] Presigned URLs para descarga directa

#### v2.0.0 - Next Generation
- [ ] WebAssembly support (cliente-lado)
- [ ] GUI web opcional
- [ ] Plugins para formatos personalizados
- [ ] Apache Iceberg tables
- [ ] Delta Lake support
- [ ] Streaming SQL queries sobre Parquet

---

## Tipos de Cambios

- `‚ú® A√±adido` - Nuevas funcionalidades
- `üöÄ Mejorado` - Mejoras en funcionalidades existentes
- `üêõ Corregido` - Correcci√≥n de bugs
- `üîí Seguridad` - Vulnerabilidades corregidas
- `üîÑ Cambios que rompen compatibilidad` - Breaking changes
- `üóëÔ∏è Deprecado` - Funcionalidades que ser√°n removidas
- `‚ùå Removido` - Funcionalidades removidas

---

## Comparaci√≥n de Versiones

### v1.0.0 vs v1.0.3 vs v1.1.0

| Caracter√≠stica | v1.0.0 | v1.0.3 | v1.1.0 |
|----------------|--------|--------|--------|
| **Formatos** | 6 | 9 | **19** |
| **Auto-detecci√≥n** | Extensi√≥n | Extensi√≥n | **Contenido** |
| **Streaming** | ‚ùå | ‚ùå | **‚úÖ** |
| **Auto-repair** | ‚ùå | ‚ùå | **‚úÖ** |
| **Auto-normalize** | ‚ùå | ‚ùå | **‚úÖ** |
| **Comandos CLI** | 2 | 5 | **7** |
| **Batch mode** | ‚ùå | ‚úÖ | **‚úÖ Mejorado** |
| **Benchmarking** | ‚ùå | ‚ùå | **‚úÖ** |
| **An√°lisis** | ‚ùå | ‚ùå | **‚úÖ** |
| **Validaci√≥n** | ‚ùå | ‚ùå | **‚úÖ** |
| **Big Data formats** | ‚ùå | ‚ùå | **‚úÖ** |
| **Estad√≠stica formats** | ‚ùå | ‚ùå | **‚úÖ** |

### L√≠neas de C√≥digo

| Versi√≥n | Python | JavaScript | Docs | Total |
|---------|--------|------------|------|-------|
| v1.0.0 | 310 | 510 | 800 | 1,620 |
| v1.0.3 | 350 | 680 | 950 | 1,980 |
| v1.1.0 | **830** | **780** | **1,200** | **2,810** |

---

## Enlaces y Recursos

- **NPM**: [ultra-parquet-converter](https://www.npmjs.com/package/ultra-parquet-converter)
- **GitHub**: [Brashkie/ultra-parquet-converter](https://github.com/Brashkie/ultra-parquet-converter)
- **Issues**: [Reportar bugs](https://github.com/Brashkie/ultra-parquet-converter/issues)
- **Discussions**: [Solicitar features](https://github.com/Brashkie/ultra-parquet-converter/discussions)

---

## Agradecimientos

### v1.1.0
Gracias a la comunidad por el feedback que gui√≥ el desarrollo de esta versi√≥n:
- Solicitudes de soporte para m√°s formatos
- Reporte de problemas con archivos grandes
- Sugerencias de auto-reparaci√≥n
- Feedback sobre UX del CLI

### Contributors
- **Brashkie** (Hepein Oficial) - Creador y mantenedor principal

---

## Notas de Release

### v1.1.0 - "Professional Edition"

Esta versi√≥n marca la evoluci√≥n de ultra-parquet-converter de una herramienta simple a una soluci√≥n profesional completa para conversi√≥n de datos.

**Highlights:**
- üéØ **19 formatos** - Cubre pr√°cticamente todos los casos de uso
- üåä **Streaming mode** - Archivos de 20GB+ ya no son problema
- üõ†Ô∏è **Auto-repair** - CSVs corruptos se arreglan autom√°ticamente
- üìä **Benchmarking** - Mide y optimiza tu pipeline

**Migration Note:**
Si vienes de v1.1.0, la √∫nica actualizaci√≥n necesaria es usar `convert` antes del nombre del archivo en CLI. El API JavaScript no tiene cambios breaking.

**Cuando actualizar:**
- ‚úÖ Si procesas archivos >1GB
- ‚úÖ Si necesitas m√°s formatos (HTML, YAML, SQLite, etc.)
- ‚úÖ Si quieres auto-reparaci√≥n de datos
- ‚úÖ Si necesitas benchmarking
- ‚ö†Ô∏è Puedes esperar si solo usas CSV/JSON b√°sico

---

**Mantenedor**: Brashkie (Hepein Oficial)  
**Email**: electronicatodo2006@gmail.com  
**√öltima actualizaci√≥n**: 25 de Noviembre, 2025  
**Licencia**: Apache-2.0
