# üöÄ Ultra Parquet Converter v1.1.0

[![npm version](https://img.shields.io/npm/v/ultra-parquet-converter.svg)](https://www.npmjs.com/package/ultra-parquet-converter)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache--2.0-yellow.svg)](https://opensource.org/licenses/Apache-2.0)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org)
[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://python.org)
[![Downloads](https://img.shields.io/npm/dm/ultra-parquet-converter.svg)](https://www.npmjs.com/package/ultra-parquet-converter)

**Conversor profesional de archivos a formato Parquet** con streaming, auto-reparaci√≥n y soporte para **19 formatos**.

Combina la velocidad de Node.js con el poder de Python + Apache Arrow para conversiones ultra-r√°pidas, procesamiento de archivos gigantes sin explotar memoria, y reparaci√≥n autom√°tica de datos corruptos.

---

## ‚ú® Caracter√≠sticas Principales

### üéØ Core Features
- üîç **Auto-detecci√≥n inteligente** - Por extensi√≥n Y contenido
- ‚ö° **Ultra-r√°pido** - Apache Arrow + Pandas optimizado
- üåä **Streaming mode** - Procesa archivos de 1GB, 5GB, 20GB+ sin explotar memoria
- üîß **Auto-reparaci√≥n** - Corrige CSVs corruptos, elimina columnas vac√≠as
- üìä **Auto-normalizaci√≥n** - Normaliza nombres, detecta tipos autom√°ticamente
- üåê **Multiplataforma** - Windows, Linux, macOS
- üêç **Python flexible** - Detecta `py`, `python`, `python3` autom√°ticamente

### üíé Advanced Features (v1.1.0)
- üîÑ **Procesamiento por chunks** - 100,000 filas por vez
- üõ†Ô∏è **Modo batch avanzado** - Convierte cientos de archivos
- üìà **Benchmarking integrado** - Mide velocidad, throughput, memoria
- üîç **An√°lisis de archivos** - Inspecciona estructura sin convertir
- ‚úÖ **Validaci√≥n Parquet** - Verifica integridad
- üì¶ **Compresi√≥n Snappy** - 50-90% reducci√≥n de tama√±o
- üé® **CLI hermoso** - Colores, spinners, estad√≠sticas detalladas

---

## üìã Formatos Soportados (19 total)

### Archivos Delimitados
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **CSV** | `.csv` | Archivos est√°ndar | ‚úÖ S√≠ |
| **TSV** | `.tsv` | Datos tabulares, exports | ‚úÖ S√≠ |
| **PSV** | `.psv` | Bases de datos Unix | ‚úÖ S√≠ |
| **DSV** | `.dsv`, `.txt`, `.log` | Delimitador desconocido | ‚úÖ S√≠ |

### Hojas de C√°lculo
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **Excel** | `.xlsx`, `.xls` | Microsoft Excel | ‚úÖ S√≠ |

### Formatos Estructurados
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **JSON** | `.json` | APIs, configuraciones | ‚úÖ S√≠ |
| **NDJSON** | `.ndjson`, `.jsonl` | JSON Lines, streaming | ‚úÖ S√≠ |
| **XML** | `.xml` | Documentos estructurados | ‚úÖ S√≠ |
| **YAML** | `.yaml`, `.yml` | Configuraciones | ‚úÖ S√≠ |
| **HTML** | `.html` | Tablas web | ‚úÖ S√≠ |

### Formatos Big Data
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **Feather** | `.feather`, `.arrow` | Apache Arrow | ‚úÖ S√≠ (magic bytes) |
| **ORC** | `.orc` | Optimized Row Columnar | ‚úÖ S√≠ (magic bytes) |
| **Avro** | `.avro` | Apache Avro | ‚úÖ S√≠ (magic bytes) |

### Bases de Datos
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **SQLite** | `.sqlite`, `.db` | Bases de datos SQLite | ‚úÖ S√≠ (magic bytes) |

### Formatos Estad√≠sticos
| Formato | Extensiones | Uso Com√∫n | Auto-detecci√≥n |
|---------|------------|-----------|----------------|
| **SPSS** | `.sav` | IBM SPSS Statistics | ‚ùå Por extensi√≥n |
| **SAS** | `.sas7bdat` | SAS datasets | ‚ùå Por extensi√≥n |
| **Stata** | `.dta` | Stata data files | ‚ùå Por extensi√≥n |

> üÜï **Novedad v1.1.0**: +10 formatos nuevos (HTML, YAML, NDJSON, Feather, ORC, Avro, SQLite, SPSS, SAS, Stata)

---

## üîß Instalaci√≥n

### Requisitos Previos

#### Node.js
```bash
# Verificar instalaci√≥n
node --version  # Requiere v18.0.0 o superior
```

#### Python
```bash
# Verificar instalaci√≥n (cualquiera de estos)
py --version       # Windows (Python Launcher)
python --version   # Windows/Linux
python3 --version  # Linux/macOS

# Debe ser Python 3.8 o superior
```

### Instalar Paquete NPM

```bash
# Global (recomendado)
npm install -g ultra-parquet-converter

# O local en tu proyecto
npm install ultra-parquet-converter
```

### Instalar Dependencias Python

```bash
# Opci√≥n 1: Autom√°tico (recomendado)
ultra-parquet-converter setup

# Opci√≥n 2: Manual
pip install -r node_modules/ultra-parquet-converter/python/requirements.txt

# En algunos sistemas:
pip3 install pandas pyarrow numpy openpyxl lxml pyyaml fastavro pyreadstat
```

---

## üöÄ Uso R√°pido

### Como CLI

```bash
# Conversi√≥n simple
ultra-parquet-converter convert archivo.csv

# Con opciones
ultra-parquet-converter convert data.json -o salida.parquet --streaming -v

# M√∫ltiples archivos
ultra-parquet-converter batch "*.csv" -o converted/

# Ver ayuda
ultra-parquet-converter --help
```

### Como Librer√≠a JavaScript

```javascript
const { convertToParquet } = require('ultra-parquet-converter');

// Conversi√≥n simple
await convertToParquet('datos.csv');

// Con opciones avanzadas
const result = await convertToParquet('huge_file.csv', {
  output: 'output.parquet',
  streaming: true,      // Para archivos grandes
  verbose: true,
  autoRepair: true,     // Corregir datos autom√°ticamente
  autoNormalize: true   // Normalizar columnas
});

console.log(`${result.rows} filas ‚Üí ${result.compression_ratio}% compresi√≥n`);
```

---

## üìö Gu√≠a Completa CLI

### Comando: `convert` - Conversi√≥n Individual

Convierte un archivo a formato Parquet.

```bash
ultra-parquet-converter convert <archivo> [opciones]
# Alias: ultra-parquet-converter c <archivo>
```

**Opciones:**
- `-o, --output <file>` - Archivo de salida personalizado
- `-v, --verbose` - Modo detallado con logs
- `--streaming` - Modo streaming para archivos >100MB
- `--no-repair` - Desactivar auto-reparaci√≥n
- `--no-normalize` - Desactivar auto-normalizaci√≥n
- `--benchmark` - Mostrar m√©tricas de performance
- `--compression <type>` - Tipo de compresi√≥n (snappy, gzip, brotli, none)

**Ejemplos:**

```bash
# B√°sico
ultra-parquet-converter convert ventas.csv

# Con salida personalizada
ultra-parquet-converter convert datos.json -o analytics/data.parquet

# Archivo grande con streaming
ultra-parquet-converter convert huge_log.csv --streaming -v

# Con benchmark
ultra-parquet-converter convert test.csv --benchmark

# Sin auto-reparaci√≥n
ultra-parquet-converter convert raw_data.csv --no-repair --no-normalize
```

**Salida ejemplo:**
```
üîÑ Ultra Parquet Converter v1.1.0

‚úì Python instalado (comando: py)
‚úì Conversi√≥n exitosa!

üìä Resultados:

   Archivo origen:  ventas.csv
   Archivo destino: ventas.parquet
   Tipo detectado:  CSV
   Filas:           125,430
   Columnas:        18
   Tama√±o original: 25.4 MB
   Tama√±o Parquet:  4.2 MB
   Compresi√≥n:      83.5%
   Tiempo:          2.34s

‚ö° Benchmark:

   Velocidad:       53,590 filas/s
   Throughput:      10.85 MB/s
```

---

### Comando: `batch` - Conversi√≥n Masiva

Convierte m√∫ltiples archivos usando patrones glob.

```bash
ultra-parquet-converter batch <patr√≥n> [opciones]
# Alias: ultra-parquet-converter b <patr√≥n>
```

**Opciones:**
- `-o, --output-dir <dir>` - Directorio de salida (default: `./output`)
- `-v, --verbose` - Modo verbose
- `--streaming` - Activar streaming para todos los archivos

**Ejemplos:**

```bash
# Todos los CSV del directorio actual
ultra-parquet-converter batch "*.csv"

# Archivos en subdirectorio
ultra-parquet-converter batch "data/*.json" -o converted/

# Con streaming y verbose
ultra-parquet-converter batch "logs/*.log" --streaming -v

# M√∫ltiples extensiones (requiere shell expansion)
ultra-parquet-converter batch "data/*.{csv,json,xlsx}"
```

**Salida ejemplo:**
```
üì¶ Ultra Parquet Converter - Modo Batch v1.1.0

Archivos encontrados: 12

‚úì ventas_2024.csv ‚Üí 82% compresi√≥n
‚úì ventas_2025.csv ‚Üí 85% compresi√≥n
‚úì productos.csv ‚Üí 91% compresi√≥n
‚úì clientes.csv ‚Üí 78% compresi√≥n
...

üìä Resumen del Batch:

   ‚úÖ Exitosos:         12
   ‚ùå Fallidos:         0
   üìÅ Total filas:      1,245,890
   üíæ Espacio ahorrado: 156.8 MB
   ‚è±Ô∏è  Tiempo total:     28.45s
   ‚ö° Velocidad media:  43,789 filas/s
```

---

### Comando: `analyze` - Analizar Archivos

Inspecciona la estructura de un archivo sin convertirlo.

```bash
ultra-parquet-converter analyze <archivo>
# Alias: ultra-parquet-converter a <archivo>
```

**Ejemplos:**

```bash
# Analizar CSV
ultra-parquet-converter analyze datos.csv

# Analizar base de datos
ultra-parquet-converter analyze database.sqlite

# Analizar archivo desconocido
ultra-parquet-converter analyze mystery_file.dat
```

**Salida ejemplo:**
```
üîç An√°lisis de Archivo

‚úì An√°lisis completado

üìã Informaci√≥n General:

   Nombre:          datos.csv
   Tipo detectado:  CSV
   Tama√±o:          25.4 MB
   Filas:           125,430
   Columnas:        18

üìä Schema:

   id                   int64
   nombre               string
   fecha                datetime
   precio               float64
   cantidad             int32
```

---

### Comando: `benchmark` - Medir Performance

Realiza pruebas de rendimiento con m√∫ltiples iteraciones.

```bash
ultra-parquet-converter benchmark <archivo> [opciones]
```

**Opciones:**
- `--iterations <n>` - N√∫mero de iteraciones (default: 3)
- `--streaming` - Probar con streaming activado

**Ejemplos:**

```bash
# Benchmark b√°sico
ultra-parquet-converter benchmark test.csv

# Con 5 iteraciones
ultra-parquet-converter benchmark large.csv --iterations 5

# Probar streaming
ultra-parquet-converter benchmark huge.csv --streaming
```

**Salida ejemplo:**
```
‚ö° Benchmark de Conversi√≥n

Archivo: test.csv
Iteraciones: 3

‚úì Iteraci√≥n 1: 2.34s
‚úì Iteraci√≥n 2: 2.28s
‚úì Iteraci√≥n 3: 2.31s

üìä Resultados:

   Filas procesadas:    125,430
   Tiempo promedio:     2.31s
   Tiempo m√≠nimo:       2.28s
   Tiempo m√°ximo:       2.34s
   Velocidad promedio:  54,285 filas/s
   Throughput:          10.99 MB/s
```

---

### Comando: `info` - Informaci√≥n de Archivo

Muestra metadatos del archivo sin procesarlo.

```bash
ultra-parquet-converter info <archivo>
# Alias: ultra-parquet-converter i <archivo>
```

**Ejemplo:**

```bash
ultra-parquet-converter info datos.csv
```

**Salida:**
```
üìã Informaci√≥n del Archivo

   Nombre:      datos.csv
   Ruta:        C:\Users\...\datos.csv
   Extensi√≥n:   .csv
   Tama√±o:      25.4 MB
   Creado:      06/11/2025
   Modificado:  25/11/2025
```

---

### Comando: `validate` - Validar Parquet

Verifica la integridad de un archivo Parquet.

```bash
ultra-parquet-converter validate <archivo.parquet>
```

**Ejemplo:**

```bash
ultra-parquet-converter validate output.parquet
```

**Salida si es v√°lido:**
```
‚úì Validaci√≥n de Parquet

‚úÖ Archivo Parquet v√°lido

üìä Informaci√≥n:

   Filas:       125,430
   Columnas:    18
   Compresi√≥n:  SNAPPY
   Versi√≥n:     2.6
```

---

### Comando: `setup` - Instalar Dependencias

Instala las dependencias Python necesarias.

```bash
ultra-parquet-converter setup
```

---

## üíª API JavaScript Detallada

### `convertToParquet(inputFile, options)`

Convierte un archivo a formato Parquet.

**Par√°metros:**
- `inputFile` (string): Ruta del archivo a convertir
- `options` (object, opcional):
  - `output` (string): Ruta del archivo de salida
  - `verbose` (boolean): Modo verbose con logs
  - `streaming` (boolean): Activar modo streaming
  - `autoRepair` (boolean): Auto-reparaci√≥n (default: true)
  - `autoNormalize` (boolean): Auto-normalizaci√≥n (default: true)

**Retorna:** `Promise<Object>`

```javascript
{
  success: true,
  input_file: "/ruta/archivo.csv",
  output_file: "/ruta/archivo.parquet",
  rows: 125430,
  columns: 18,
  input_size: 26632192,      // bytes
  output_size: 4398046,      // bytes
  compression_ratio: 83.48,  // porcentaje
  file_type: "csv",
  elapsed_time: 2.34,        // segundos
  chunks_processed: 2,       // si streaming
  errors_fixed: 5,           // si auto-repair
  columns_removed: 3,        // si auto-normalize
  streaming_mode: false
}
```

**Ejemplos:**

```javascript
const { convertToParquet } = require('ultra-parquet-converter');

// 1. Conversi√≥n simple
const result = await convertToParquet('datos.csv');
console.log(`‚úì ${result.rows} filas convertidas`);

// 2. Con todas las opciones
const result = await convertToParquet('input.json', {
  output: 'output/data.parquet',
  verbose: true,
  streaming: false,
  autoRepair: true,
  autoNormalize: true
});

// 3. Archivo gigante (20GB)
const result = await convertToParquet('huge_file.csv', {
  streaming: true,  // ¬°No explota la memoria!
  verbose: true
});

console.log(`Procesados ${result.chunks_processed} chunks`);
console.log(`Velocidad: ${Math.round(result.rows / result.elapsed_time)} filas/s`);
```

---

### `analyzeFile(filePath)`

Analiza la estructura de un archivo.

**Retorna:** `Promise<Object>`

```javascript
const { analyzeFile } = require('ultra-parquet-converter');

const analysis = await analyzeFile('datos.csv');

console.log(`Tipo: ${analysis.detected_type}`);
console.log(`Filas: ${analysis.rows}`);
console.log(`Columnas: ${analysis.columns}`);
```

---

### `benchmarkConversion(filePath, options)`

Realiza benchmark de conversi√≥n.

**Retorna:** `Promise<Object>`

```javascript
const { benchmarkConversion } = require('ultra-parquet-converter');

const benchmark = await benchmarkConversion('test.csv', {
  streaming: false
});

const speed = Math.round(benchmark.rows / benchmark.elapsed_time);
console.log(`Velocidad: ${speed} filas/s`);
```

---

### `validateParquet(filePath)`

Valida un archivo Parquet.

**Retorna:** `Promise<Object>`

```javascript
const { validateParquet } = require('ultra-parquet-converter');

const validation = await validateParquet('output.parquet');

if (validation.valid) {
  console.log(`‚úì V√°lido: ${validation.rows} filas`);
} else {
  console.error(`‚úó Error: ${validation.error}`);
}
```

---

### `checkPythonSetup()`

Verifica instalaci√≥n de Python.

**Retorna:** `Promise<Object>`

```javascript
const { checkPythonSetup } = require('ultra-parquet-converter');

const check = await checkPythonSetup();

if (check.installed) {
  console.log(`‚úì ${check.message}`);
} else {
  console.error(`‚úó Python no encontrado`);
}
```

---

## üî• Casos de Uso Reales

### 1. Data Engineering - Pipeline ETL

```javascript
// Pipeline completo: CSV ‚Üí limpieza ‚Üí Parquet
const { convertToParquet } = require('ultra-parquet-converter');
const path = require('path');
const fs = require('fs');

async function etlPipeline(inputDir, outputDir) {
  const files = fs.readdirSync(inputDir)
    .filter(f => f.endsWith('.csv'));
  
  console.log(`Procesando ${files.length} archivos...`);
  
  for (const file of files) {
    const inputPath = path.join(inputDir, file);
    const outputPath = path.join(outputDir, file.replace('.csv', '.parquet'));
    
    const result = await convertToParquet(inputPath, {
      output: outputPath,
      streaming: true,      // Archivos grandes
      autoRepair: true,     // Limpia datos
      autoNormalize: true   // Normaliza columnas
    });
    
    console.log(`‚úì ${file}: ${result.compression_ratio}% compresi√≥n`);
  }
}

etlPipeline('./raw-data', './processed');
```

---

### 2. Data Science - Preparaci√≥n de Datasets ML

```javascript
// Preprocesa datasets para entrenamiento
const datasets = ['train.csv', 'test.csv', 'validation.csv'];

for (const dataset of datasets) {
  const result = await convertToParquet(`data/${dataset}`, {
    output: `data/parquet/${dataset.replace('.csv', '.parquet')}`,
    autoRepair: true,        // Elimina valores nulos
    autoNormalize: true      // Normaliza features
  });
  
  console.log(`${dataset}:`);
  console.log(`  Errores corregidos: ${result.errors_fixed}`);
  console.log(`  Columnas eliminadas: ${result.columns_removed}`);
  console.log(`  Compresi√≥n: ${result.compression_ratio}%`);
}
```

---

### 3. Analytics - Optimizaci√≥n de Reportes

```bash
# Script Bash para analytics diarios
#!/bin/bash

# Convierte reportes Excel pesados
ultra-parquet-converter batch "reports/daily/*.xlsx" -o analytics/parquet/

# Convierte logs del servidor
ultra-parquet-converter convert logs/access.log \
  --streaming \
  -o analytics/access.parquet

echo "‚úì Reportes optimizados - Consultas 100x m√°s r√°pidas"
```

---

### 4. Archivado - Compresi√≥n Masiva

```javascript
// Archiva datos hist√≥ricos con m√°xima compresi√≥n
const { convertToParquet } = require('ultra-parquet-converter');
const glob = require('glob');

async function archiveHistoricalData() {
  const files = glob.sync('archive/**/*.csv');
  
  console.log(`Archivando ${files.length} archivos hist√≥ricos...`);
  
  let totalSaved = 0;
  
  for (const file of files) {
    const result = await convertToParquet(file, {
      output: file.replace('.csv', '.parquet'),
      streaming: true
    });
    
    totalSaved += (result.input_size - result.output_size);
  }
  
  console.log(`Espacio ahorrado: ${(totalSaved / 1024 / 1024 / 1024).toFixed(2)} GB`);
}

archiveHistoricalData();
```

---

### 5. Procesamiento de Logs - Streaming

```javascript
// Procesa logs gigantes (50GB) sin explotar memoria
const result = await convertToParquet('server-logs-2025.log', {
  output: 'logs/2025.parquet',
  streaming: true,      // ¬°CR√çTICO para archivos grandes!
  verbose: true,
  autoRepair: true      // Salta l√≠neas corruptas
});

console.log(`Procesados ${result.chunks_processed} chunks`);
console.log(`${result.rows.toLocaleString()} l√≠neas de log`);
console.log(`Tiempo: ${result.elapsed_time}s`);
console.log(`Memoria usada: <300MB (gracias a streaming)`);
```

---

### 6. Migraci√≥n de Bases de Datos

```javascript
// SQLite ‚Üí Parquet
const result = await convertToParquet('production.sqlite', {
  output: 'backup/production.parquet',
  verbose: true
});

console.log(`Base de datos migrada:`);
console.log(`  Filas: ${result.rows.toLocaleString()}`);
console.log(`  Tama√±o SQLite: ${(result.input_size / 1024 / 1024).toFixed(2)} MB`);
console.log(`  Tama√±o Parquet: ${(result.output_size / 1024 / 1024).toFixed(2)} MB`);
console.log(`  Reducci√≥n: ${result.compression_ratio}%`);
```

---

### 7. Web Scraping - Tablas HTML

```javascript
// Extrae tablas de HTML y convierte a Parquet
const result = await convertToParquet('scraping/wikipedia-table.html', {
  output: 'data/wikipedia.parquet'
});

console.log(`Tabla extra√≠da: ${result.rows} filas`);
```

---

### 8. Configuraciones YAML ‚Üí Parquet

```javascript
// Convierte configuraciones para an√°lisis
await convertToParquet('configs/app-config.yaml', {
  output: 'analytics/configs.parquet'
});
```

---

## üìä Benchmarks y Performance

### Archivos Peque√±os (<100MB)

| Tama√±o | Filas | Formato | Tiempo | Velocidad | Compresi√≥n |
|--------|-------|---------|--------|-----------|------------|
| 10 MB | 100K | CSV | 0.8s | 125K filas/s | 82% |
| 25 MB | 250K | JSON | 1.2s | 208K filas/s | 75% |
| 50 MB | 500K | XLSX | 3.5s | 143K filas/s | 88% |

### Archivos Medianos (100MB-1GB)

| Tama√±o | Filas | Formato | Tiempo | Velocidad | Memoria |
|--------|-------|---------|--------|-----------|---------|
| 100 MB | 1M | CSV | 4.2s | 238K filas/s | 85 MB |
| 500 MB | 5M | TSV | 22s | 227K filas/s | 180 MB |
| 1 GB | 10M | CSV | 45s | 222K filas/s | 250 MB* |

*Con streaming activado

### Archivos Grandes (1GB-20GB) con Streaming

| Tama√±o | Filas | Formato | Tiempo | Velocidad | Memoria |
|--------|-------|---------|--------|-----------|---------|
| 5 GB | 50M | CSV | 4m 30s | 185K filas/s | 280 MB |
| 10 GB | 100M | LOG | 8m 15s | 202K filas/s | 290 MB |
| 20 GB | 200M | TSV | 16m 40s | 200K filas/s | 300 MB |

**üî• Sin streaming, estos archivos causar√≠an "Out of Memory"**

### Comparaci√≥n: Streaming vs Normal

| Archivo | Modo Normal | Modo Streaming |
|---------|-------------|----------------|
| 5 GB CSV | ‚ùå Out of Memory | ‚úÖ 280 MB RAM |
| 10 GB LOG | ‚ùå Crash | ‚úÖ 290 MB RAM |
| 20 GB TSV | ‚ùå Imposible | ‚úÖ 300 MB RAM |

---

## üõ†Ô∏è Caracter√≠sticas Avanzadas

### Auto-reparaci√≥n de Datos

El conversor detecta y corrige autom√°ticamente:

‚úÖ **Columnas vac√≠as** - Eliminadas autom√°ticamente
```
Entrada: id, nombre, vacio1, edad, vacio2
Salida:  id, nombre, edad
```

‚úÖ **Tipos incorrectos** - Detecta y convierte
```
Entrada: "123" (string)
Salida:  123 (int64)
```

‚úÖ **Filas duplicadas** - Eliminadas
```
Entrada: 10,000 filas (500 duplicadas)
Salida:  9,500 filas √∫nicas
```

‚úÖ **CSVs corruptos** - Salta l√≠neas malas
```
L√≠nea 1: "a","b","c"        ‚úì
L√≠nea 2: "1","2"            ‚úó Saltada
L√≠nea 3: "3","4","5"        ‚úì
```

**Desactivar auto-reparaci√≥n:**
```bash
ultra-parquet-converter convert data.csv --no-repair
```

---

### Auto-normalizaci√≥n

Normaliza autom√°ticamente:

‚úÖ **Nombres de columnas**
```
"Cliente ID" ‚Üí "cliente_id"
"Fecha Venta" ‚Üí "fecha_venta"
"PRECIO TOTAL" ‚Üí "precio_total"
```

‚úÖ **Columnas constantes** - Eliminadas
```
Columna "status" = "active" en todas las filas
‚Üí Eliminada (ahorra espacio)
```

**Desactivar auto-normalizaci√≥n:**
```bash
ultra-parquet-converter convert data.csv --no-normalize
```

---

### Modo Streaming Inteligente

El modo streaming se activa:

1. **Manualmente** con `--streaming`
2. **Autom√°ticamente** si el archivo >100MB

**¬øC√≥mo funciona?**
- Procesa 100,000 filas por chunk
- Escribe incrementalmente a Parquet
- Memoria constante (~300MB)

```javascript
// Archivo de 50GB
const result = await convertToParquet('huge.csv', {
  streaming: true
});

console.log(`Procesados ${result.chunks_processed} chunks`);
// Ej: 500 chunks de 100K filas cada uno
```

---

### Auto-detecci√≥n de Formato

Detecta formatos por:

**1. Extensi√≥n (r√°pido)**
```
archivo.csv ‚Üí CSV
archivo.json ‚Üí JSON
```

**2. Contenido (inteligente)**
```
archivo.txt con comas ‚Üí CSV
archivo.dat con tabs ‚Üí TSV
archivo.data con magic "SQLite" ‚Üí SQLite
```

**Magic bytes detectados:**
- SQLite: `SQLite format 3`
- Parquet: `PAR1`
- Arrow/Feather: `ARROW1`
- ORC: `ORC`
- Avro: `Obj\x01`

---

## üéØ Ventajas de Parquet

| Caracter√≠stica | CSV | JSON | Excel | Parquet |
|----------------|-----|------|-------|---------|
| **Tama√±o** | 100% | 120% | 80% | **15-30%** ‚ö° |
| **Velocidad lectura** | 1x | 0.8x | 0.5x | **10-100x** ‚ö° |
| **Compresi√≥n** | ‚ùå | ‚ùå | ‚úÖ | **‚úÖ‚úÖ‚úÖ** |
| **Schema** | ‚ùå | Parcial | ‚úÖ | **‚úÖ Fuerte** |
| **Columnar** | ‚ùå | ‚ùå | ‚ùå | **‚úÖ** |
| **Big Data** | Lento | Lento | ‚ùå | **‚úÖ Optimizado** |

**Ejemplo real:**
```
CSV:    100 MB
JSON:   120 MB
Excel:   80 MB
Parquet: 18 MB  ‚Üê 82% m√°s peque√±o
```

---

## üêõ Soluci√≥n de Problemas

### Error: "Python no encontrado"

```bash
# Verifica instalaci√≥n
py --version       # Windows
python --version   # Linux/Windows
python3 --version  # Linux/macOS

# Si no est√° instalado:
# Windows: https://python.org (marca "Add to PATH")
# macOS: brew install python
# Ubuntu: sudo apt install python3 python3-pip
```

---

### Error: "Dependencias Python faltantes"

```bash
# Reinstalar autom√°ticamente
ultra-parquet-converter setup

# O manualmente
pip install pandas pyarrow numpy openpyxl lxml pyyaml fastavro pyreadstat

# Linux/macOS puede necesitar pip3
pip3 install -r python/requirements.txt
```

---

### Error: "Out of Memory"

```bash
# Activar modo streaming
ultra-parquet-converter convert huge_file.csv --streaming

# O aumentar memoria Node.js
NODE_OPTIONS="--max-old-space-size=4096" ultra-parquet-converter convert file.csv
```

---

### Error: "MODULE_NOT_FOUND"

```bash
# Instalar dependencias Node.js
npm install

# Si instalaste globalmente
npm install -g ultra-parquet-converter
```

---

### CSV corrupto no se convierte

```bash
# El auto-repair deber√≠a solucionarlo, pero si no:
# 1. Verifica encoding (debe ser UTF-8)
# 2. Activa verbose para ver errores
ultra-parquet-converter convert broken.csv -v

# 3. Si falla, reporta el issue con el archivo de ejemplo
```

---

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas!

### Reportar Bugs
1. Busca issues existentes
2. Crea un nuevo issue con:
   - Versi√≥n de ultra-parquet-converter
   - Versi√≥n de Node.js y Python
   - Sistema operativo
   - Archivo de ejemplo (si es posible)
   - Comando exacto usado
   - Error completo

### Solicitar Features
1. Abre un issue con etiqueta "enhancement"
2. Describe el caso de uso
3. Prop√≥n la API/sintaxis

### Pull Requests
1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/MiFeature`
3. Commit: `git commit -m 'feat: a√±adir MiFeature'`
4. Push: `git push origin feature/MiFeature`
5. Abre un Pull Request

Ver [CONTRIBUTING.md](CONTRIBUTING.md) para m√°s detalles.

---

## üìù Licencia

Apache-2.0 License - ver [LICENSE](LICENSE)

---

## üôè Agradecimientos

### Tecnolog√≠as Core
- **Apache Arrow** - Motor columnar ultra-r√°pido
- **Pandas** - Manipulaci√≥n de datos en Python
- **PyArrow** - Interfaz Python para Arrow
- **NumPy** - Computaci√≥n num√©rica

### Formatos Espec√≠ficos
- **openpyxl** - Lectura Excel
- **lxml** - Parsing XML/HTML
- **PyYAML** - Parsing YAML
- **fastavro** - Apache Avro
- **pyreadstat** - SPSS, SAS, Stata

### CLI y UX
- **Commander.js** - CLI framework
- **Chalk** - Colores en terminal
- **Ora** - Spinners animados

---

## üìß Soporte y Contacto

### üêõ Reportar Issues
- [GitHub Issues](https://github.com/Brashkie/ultra-parquet-converter/issues)

### üí° Solicitar Features
- [GitHub Discussions](https://github.com/Brashkie/ultra-parquet-converter/discussions)

### üìß Contacto Directo
- **Email**: electronicatodo2006@gmail.com
- **Creador**: Hepein Oficial x Brashkie

---

## üìà Roadmap

### v1.3.0 (Pr√≥xima)
- [ ] Parallel processing (multi-thread)
- [ ] GPU acceleration (cuDF)
- [ ] Compresi√≥n adaptativa (elige mejor algoritmo)
- [ ] Progress bar para archivos grandes
- [ ] Modo watch (monitoreo autom√°tico)

### v1.4.0
- [ ] REST API server
- [ ] WebAssembly support
- [ ] Cloud integration (S3, GCS, Azure)
- [ ] GUI web opcional

### v2.0.0
- [ ] Plugins para formatos personalizados
- [ ] Soporte para Iceberg tables
- [ ] Delta Lake support
- [ ] Streaming SQL queries

---

## ‚≠ê Estad√≠sticas

- **Formatos soportados**: 19
- **L√≠neas de c√≥digo**: ~2,500
- **Tests**: Integraci√≥n completa
- **Plataformas**: Windows, Linux, macOS
- **Python**: 3.8+
- **Node.js**: 18+

---

## üèÜ Comparaci√≥n con Alternativas

| Feature | ultra-parquet-converter | pandas.to_parquet | other-converter |
|---------|------------------------|-------------------|-----------------|
| Formatos | **19** | 1 | 6 |
| Streaming | **‚úÖ** | ‚ùå | ‚ùå |
| Auto-repair | **‚úÖ** | ‚ùå | ‚ùå |
| CLI | **‚úÖ Avanzado** | ‚ùå | ‚úÖ B√°sico |
| Auto-detecci√≥n | **‚úÖ Contenido** | ‚ùå | ‚úÖ Extensi√≥n |
| Benchmarking | **‚úÖ** | ‚ùå | ‚ùå |
| Batch | **‚úÖ** | ‚ùå | ‚úÖ |

---

**Hecho con ‚ù§Ô∏è para la comunidad de Data Engineering**

**Creador: Hepein Oficial x Brashkie**

‚≠ê Si te gusta este proyecto, [dale una estrella en GitHub](https://github.com/Brashkie/ultra-parquet-converter)!

---

**Versi√≥n**: 1.1.0  
**√öltima actualizaci√≥n**: 25 de Noviembre, 2025  
**Licencia**: Apache-2.0  
**NPM**: [ultra-parquet-converter](https://www.npmjs.com/package/ultra-parquet-converter)
